<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>How We Design Tests | Unitree G1 Knowledge Hub</title>
    <meta
      name="description"
      content="Methodology for Unitree G1 retrieval and Llama source-selection benchmarks."
    />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@400;500;700&family=IBM+Plex+Mono:wght@400;500&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="./assets/styles.css" />
  </head>
  <body>
    <div class="bg-orb orb-a"></div>
    <div class="bg-orb orb-b"></div>

    <main class="page">
      <nav class="topnav mono reveal">
        <a href="./index.html">Overview</a>
        <a href="./how-we-design.html" class="active">How We Design</a>
        <a href="./examples.html">Examples</a>
      </nav>

      <header class="hero reveal delay-1">
        <p class="eyebrow">Methodology</p>
        <h1>How we design retrieval and Llama benchmark tests</h1>
        <p class="subtext">
          North-star goal: make this repository the best in-repo workspace for Codex-style coding
          agents to handle Unitree G1 questions with grounded citations.
        </p>
      </header>

      <section class="panel reveal delay-2 prose">
        <h2>North Star</h2>
        <ul>
          <li>Codex can run inside this repo and answer most G1 questions without external guessing.</li>
          <li>Answers always cite local files/URLs and separate Verified facts from Inference.</li>
          <li>Benchmarks expose failures clearly so we can improve docs, indexing, and skill rules.</li>
        </ul>
      </section>

      <section class="panel reveal delay-2 prose">
        <h2>Evaluation Architecture</h2>
        <ol>
          <li>Sync and verify sources from manifest and official repos.</li>
          <li>Build a local index (<code>data/index/knowledge_index.jsonl</code>).</li>
          <li>Evaluate lexical retrieval on benchmark question cases.</li>
          <li>Evaluate model source-selection from retrieved candidates.</li>
          <li>Apply benchmark leakage guard (exclude benchmark YAML unless a case expects it).</li>
          <li>Track quality gates and failed case IDs to guide improvements.</li>
        </ol>
      </section>

      <section class="panel reveal delay-3 prose">
        <h2>Question Design Rules</h2>
        <p>
          Questions are generated with Llama and curated by Codex into a controlled YAML benchmark.
          Each question includes:
        </p>
        <ul>
          <li><code>id</code> in snake_case</li>
          <li>natural-language question text</li>
          <li><code>expected_path_patterns</code> (1-2 target evidence paths)</li>
          <li><code>difficulty</code> (easy/medium/hard)</li>
        </ul>
        <p>
          Coverage areas: SDK2, SDK2 Python, DDS interfaces, ROS2 examples, sim2sim/sim2real,
          deployment strategy, verification status, repo lock/coverage, and skill usage.
        </p>
      </section>

      <section class="panel reveal delay-3 prose">
        <h2>Scoring Logic</h2>
        <h3>Retriever Test</h3>
        <ul>
          <li>Top-K lexical search over the local index with intent-aware scoring (default K=8).</li>
          <li>Results are path-deduplicated to avoid chunk-level duplicates inflating rankings.</li>
          <li>A case passes if any expected path pattern appears in top-K results.</li>
          <li>Source types are weighted (<code>support_doc</code>, <code>curated_doc</code>, <code>skill_doc</code>, etc.).</li>
          <li>Optional hard-negative fields (<code>forbidden_path_patterns</code>, <code>require_all_expected</code>) stress-test ranking robustness.</li>
          <li>Strict mode fails CI/local run when pass rate is below configured threshold.</li>
        </ul>
        <h3>Llama Source-Selection Test</h3>
        <ul>
          <li>Retriever first gathers candidates (top-K then dedup to candidate set).</li>
          <li>Llama must return JSON with selected paths and rationale only.</li>
          <li>A case passes if at least one selected path matches expected patterns.</li>
          <li>Precision and recall are computed per case and aggregated.</li>
        </ul>
      </section>

      <section class="panel reveal delay-3 prose">
        <h2>Codex Stretch Experiments</h2>
        <p>
          Beyond baseline and curated sets, we run a Codex-first stretch benchmark focused on
          workflow understanding, answer-contract compliance, and repo-navigation tasks.
        </p>
        <ul>
          <li><code>make eval-retrieval-codex-stretch</code></li>
          <li><code>make eval-agent-ollama-codex-stretch</code></li>
        </ul>
        <p>
          Design details are tracked in
          <code>docs/verification/codex_agent_experiments.md</code>.
        </p>
        <h3>Hard-Negative Follow-up</h3>
        <p>
          We also run a hard-negative retrieval track to catch ranking noise and leakage behavior:
        </p>
        <ul>
          <li><code>make eval-retrieval-codex-hardneg</code></li>
          <li><code>make eval-agent-ollama-codex-hardneg</code></li>
        </ul>
      </section>

      <section class="panel reveal delay-3 prose">
        <h2>Why This Repo Setup Helps Codex and Agents</h2>
        <ul>
          <li>
            <code>AGENTS.md</code> defines a deterministic workflow: sync, verify, retrieve, and
            answer with citations.
          </li>
          <li>
            <code>skills/unitree-g1-expert/SKILL.md</code> enforces grounding priority and
            Verified/Inference separation.
          </li>
          <li>
            Verification artifacts (`g1_docs_verification`, `repo_lock`, `coverage_report`) add
            environment-aware reliability constraints.
          </li>
          <li>
            Benchmarks include skill-usage and ops-level questions so agent behavior is tested on
            practical, not purely synthetic, tasks.
          </li>
        </ul>
        <p class="hint mono">
          Important caveat: support.unitree.com G1 pages are currently blocked in this environment;
          this is surfaced in verification reports and site verdicts.
        </p>
      </section>

      <section class="panel reveal delay-3">
        <div class="panel-head">
          <h2>Current Gates and Results</h2>
          <span class="hint">Loaded from latest verification artifacts</span>
        </div>
        <div id="designBenchSummary" class="bench-grid"></div>
        <h3 class="section-subhead">Test Commands</h3>
        <div id="designTests" class="tests-list"></div>
      </section>

      <footer class="foot mono">
        Method page source: scripts/eval_retrieval.py, scripts/eval_openai_compatible.py,
        scripts/generate_question_bank.py.
      </footer>
    </main>

    <script src="./assets/how-design.js"></script>
  </body>
</html>

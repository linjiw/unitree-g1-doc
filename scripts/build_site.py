#!/usr/bin/env python3
"""Build static data payloads for the GitHub Pages demo website."""

from __future__ import annotations

import argparse
import json
import time
from pathlib import Path
from typing import Any

try:
    import yaml
except ImportError as exc:  # pragma: no cover - runtime dependency check
    raise SystemExit(
        "Missing dependency: pyyaml. Install with `pip install pyyaml`."
    ) from exc


EXAMPLE_CASE_ORDER = [
    "g1_remote_vs_onboard",
    "g1_sim2real_pipeline",
    "g1_support_doc_status",
    "g1_skill_usage",
    "g1_repo_lock_intent",
]


CODEX_EXAMPLE_ANSWERS: dict[str, dict[str, Any]] = {
    "g1_remote_vs_onboard": {
        "summary": "Start with remote PC inference, then move onboard only after timing and thermal headroom are proven.",
        "verified": [
            "The deployment guide recommends remote PC inference as the default first step.",
            "Onboard inference is positioned as a later step after control-loop and thermal validation.",
        ],
        "inference": [
            "For heavier models, remote deployment usually reduces early bring-up risk while preserving faster iteration.",
        ],
        "citations": [
            "docs/pipelines/deployment-onboard-vs-remote-pc.md",
            "docs/pipelines/sim2sim-sim2real.md",
        ],
    },
    "g1_sim2real_pipeline": {
        "summary": "Use a staged flow: simulation training, sim2sim validation, then conservative sim2real bring-up.",
        "verified": [
            "The workflow defines Stage 1 (simulation), Stage 2 (sim2sim checks), and Stage 3 (sim2real bring-up).",
            "The same document recommends conservative command envelopes and verifying DDS/SDK communication before full policy loop deployment.",
        ],
        "inference": [
            "In practice, teams should keep remote-PC inference as a default path until onboard latency and thermal limits are validated for the model.",
        ],
        "citations": [
            "docs/pipelines/sim2sim-sim2real.md",
            "data/repos/unitree_rl_gym/deploy/deploy_real/README.md",
        ],
    },
    "g1_support_doc_status": {
        "summary": "Support-site verification currently reports blocked access for all discovered G1 developer URLs in this environment.",
        "verified": [
            "The verification report records 5 discovered URLs, 0 verified pages, and 5 blocked_access entries.",
            "Coverage documents should be consulted alongside verification status before high-confidence claims.",
        ],
        "inference": [
            "When official support pages are blocked, confidence should shift toward mirrored repos plus curated docs while stating the verification limitation.",
        ],
        "citations": [
            "docs/verification/g1_docs_verification.md",
            "docs/verification/coverage_report.md",
        ],
    },
    "g1_skill_usage": {
        "summary": "Follow AGENTS + SKILL workflow: refresh sources, query index, then answer with explicit citations and Verified/Inference separation.",
        "verified": [
            "AGENTS.md defines the mandatory refresh, verification, retrieval, and response format workflow.",
            "The skill and AI quickstart reinforce local evidence retrieval and citation-first output.",
        ],
        "inference": [
            "This structure reduces hallucination risk because agents must anchor claims to local files and measurable retrieval quality checks.",
        ],
        "citations": [
            "AGENTS.md",
            "skills/unitree-g1-expert/SKILL.md",
            "docs/ai-agent-quickstart.md",
        ],
    },
    "g1_repo_lock_intent": {
        "summary": "The repo_lock report is documented in docs/verification/repo_lock.md and generated by scripts/build_repo_lock.py.",
        "verified": [
            "Repo lock markdown reports total repos, working clone presence, mirror presence, and HEAD values.",
            "The build script explicitly writes repo_lock JSON and markdown outputs under docs/verification.",
        ],
        "inference": [
            "Because this query failed in the current benchmark run, retrieval ranking needs tuning for repo-lock specific intents.",
        ],
        "citations": [
            "docs/verification/repo_lock.md",
            "scripts/build_repo_lock.py",
        ],
    },
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build static site data files")
    parser.add_argument(
        "--index-jsonl",
        type=Path,
        default=Path("data/index/knowledge_index.jsonl"),
        help="Knowledge index JSONL path",
    )
    parser.add_argument(
        "--index-meta",
        type=Path,
        default=Path("data/index/knowledge_index.meta.json"),
        help="Knowledge index metadata path",
    )
    parser.add_argument(
        "--manifest",
        type=Path,
        default=Path("sources/unitree_g1_sources.yaml"),
        help="Source manifest path",
    )
    parser.add_argument(
        "--verification-json",
        type=Path,
        default=Path("docs/verification/g1_docs_verification.json"),
        help="Verification report path",
    )
    parser.add_argument(
        "--repo-lock-json",
        type=Path,
        default=Path("docs/verification/repo_lock.json"),
        help="Repo lock report path",
    )
    parser.add_argument(
        "--retrieval-eval-json",
        type=Path,
        default=Path("docs/verification/retrieval_eval.json"),
        help="Baseline retrieval evaluation JSON path",
    )
    parser.add_argument(
        "--agent-eval-json",
        type=Path,
        default=Path("docs/verification/agent_eval.json"),
        help="Baseline agent source-selection evaluation JSON path",
    )
    parser.add_argument(
        "--ollama-retrieval-eval-json",
        type=Path,
        default=Path("docs/verification/ollama_question_retrieval_eval.json"),
        help="Ollama question benchmark retrieval evaluation JSON path",
    )
    parser.add_argument(
        "--ollama-agent-eval-json",
        type=Path,
        default=Path("docs/verification/ollama_agent_eval.json"),
        help="Ollama question benchmark agent source-selection evaluation JSON path",
    )
    parser.add_argument(
        "--codex-stretch-retrieval-eval-json",
        type=Path,
        default=Path("docs/verification/codex_stretch_retrieval_eval.json"),
        help="Codex stretch retrieval evaluation JSON path",
    )
    parser.add_argument(
        "--codex-stretch-agent-eval-json",
        type=Path,
        default=Path("docs/verification/codex_stretch_agent_eval.json"),
        help="Codex stretch agent source-selection evaluation JSON path",
    )
    parser.add_argument(
        "--ollama-question-bank-yaml",
        type=Path,
        default=Path("benchmarks/ollama_question_bank.yaml"),
        help="Ollama question bank YAML path",
    )
    parser.add_argument(
        "--out-dir",
        type=Path,
        default=Path("site/data"),
        help="Output directory for site JSON files",
    )
    parser.add_argument(
        "--max-records",
        type=int,
        default=5000,
        help="Maximum number of search records emitted to site",
    )
    return parser.parse_args()


def read_json(path: Path) -> dict[str, Any]:
    if not path.exists():
        return {}
    return json.loads(path.read_text(encoding="utf-8"))


def read_yaml(path: Path) -> dict[str, Any]:
    if not path.exists():
        return {}
    payload = yaml.safe_load(path.read_text(encoding="utf-8"))
    return payload if isinstance(payload, dict) else {}


def summarize_eval(path: Path, label: str, test_type: str, command: str) -> dict[str, Any]:
    payload = read_json(path)
    if not payload:
        return {
            "label": label,
            "test_type": test_type,
            "command": command,
            "path": str(path),
            "available": False,
        }

    results = payload.get("results", [])
    if isinstance(results, list) and results:
        endpoint_errors = 0
        for item in results:
            if not isinstance(item, dict):
                continue
            err = str(item.get("error", ""))
            if "Failed to reach model endpoint" in err:
                endpoint_errors += 1
        if endpoint_errors == len(results):
            return {
                "label": label,
                "test_type": test_type,
                "command": command,
                "path": str(path),
                "available": False,
                "unavailable_reason": "model_endpoint_unreachable",
            }

    failed_cases: list[str] = []
    if isinstance(results, list):
        for item in results:
            if not isinstance(item, dict):
                continue
            if bool(item.get("pass")):
                continue
            case_id = item.get("id")
            if case_id:
                failed_cases.append(str(case_id))

    return {
        "label": label,
        "test_type": test_type,
        "command": command,
        "path": str(path),
        "available": True,
        "timestamp_unix": payload.get("timestamp_unix"),
        "benchmark": payload.get("benchmark"),
        "index": payload.get("index"),
        "model": payload.get("model"),
        "api_base": payload.get("api_base"),
        "total": payload.get("total"),
        "passed": payload.get("passed"),
        "pass_rate": payload.get("pass_rate"),
        "top_k": payload.get("top_k"),
        "fail_below": payload.get("fail_below"),
        "avg_precision": payload.get("avg_precision"),
        "avg_recall": payload.get("avg_recall"),
        "failed_cases": failed_cases,
        "failed_count": len(failed_cases),
    }


def load_question_bank(path: Path) -> dict[str, dict[str, str]]:
    payload = read_yaml(path)
    questions = payload.get("questions", []) if isinstance(payload, dict) else []
    out: dict[str, dict[str, str]] = {}
    if not isinstance(questions, list):
        return out

    for item in questions:
        if not isinstance(item, dict):
            continue
        qid = str(item.get("id", "")).strip()
        if not qid:
            continue
        out[qid] = {
            "question": str(item.get("question", "")).strip(),
            "difficulty": str(item.get("difficulty", "")).strip(),
        }
    return out


def unique_top_paths(top_results: list[Any], limit: int = 3) -> list[str]:
    out: list[str] = []
    seen: set[str] = set()
    for item in top_results:
        if not isinstance(item, dict):
            continue
        path = str(item.get("path", "")).strip()
        if not path or path in seen:
            continue
        seen.add(path)
        out.append(path)
        if len(out) >= limit:
            break
    return out


def generic_codex_answer(
    case_id: str,
    expected_paths: list[str],
    top_paths: list[str],
) -> dict[str, Any]:
    primary = top_paths[0] if top_paths else ""
    verified_line = (
        f"Primary evidence path for this query is `{primary}`."
        if primary
        else "Use the highest-ranked local path from query_index output as the primary citation."
    )
    return {
        "summary": f"Use local retrieval evidence to answer case `{case_id}` with explicit citations.",
        "verified": [verified_line],
        "inference": [
            "If expected source paths are missing, refresh sources and rebuild the index before final answering."
        ],
        "citations": expected_paths[:2] or top_paths[:2],
    }


def build_benchmark_examples(
    *,
    retrieval_path: Path,
    agent_path: Path,
    question_bank_path: Path,
) -> dict[str, Any]:
    retrieval_payload = read_json(retrieval_path)
    agent_payload = read_json(agent_path)
    qbank = load_question_bank(question_bank_path)

    retrieval_results = retrieval_payload.get("results", []) if isinstance(retrieval_payload, dict) else []
    agent_results = agent_payload.get("results", []) if isinstance(agent_payload, dict) else []

    retrieval_map = {
        str(item.get("id")): item
        for item in retrieval_results
        if isinstance(item, dict) and item.get("id")
    }
    agent_map = {
        str(item.get("id")): item
        for item in agent_results
        if isinstance(item, dict) and item.get("id")
    }

    examples: list[dict[str, Any]] = []
    for case_id in EXAMPLE_CASE_ORDER:
        retr_case = retrieval_map.get(case_id, {})
        agent_case = agent_map.get(case_id, {})
        if not retr_case and not agent_case:
            continue

        query = str(agent_case.get("query") or retr_case.get("query") or qbank.get(case_id, {}).get("question", ""))
        expected_paths = list(agent_case.get("expected_path_patterns") or retr_case.get("expected_path_patterns") or [])
        top_paths = unique_top_paths(list(retr_case.get("top_results", [])))

        codex_answer = CODEX_EXAMPLE_ANSWERS.get(case_id) or generic_codex_answer(case_id, expected_paths, top_paths)

        examples.append(
            {
                "id": case_id,
                "query": query,
                "difficulty": qbank.get(case_id, {}).get("difficulty", ""),
                "expected_path_patterns": expected_paths,
                "retrieval": {
                    "pass": bool(retr_case.get("pass")),
                    "reason": str(retr_case.get("reason", "")),
                    "top_paths": top_paths,
                },
                "llama": {
                    "model": str(agent_payload.get("model", "")),
                    "pass": bool(agent_case.get("pass")),
                    "precision": agent_case.get("precision"),
                    "recall": agent_case.get("recall"),
                    "selected_paths": list(agent_case.get("selected_paths", [])),
                    "rationale": str(agent_case.get("rationale", "")),
                },
                "codex": codex_answer,
            }
        )

    return {
        "generated_at_unix": int(time.time()),
        "benchmark": str(agent_payload.get("benchmark") or retrieval_payload.get("benchmark") or ""),
        "model": str(agent_payload.get("model", "")),
        "examples": examples,
    }


def main() -> int:
    args = parse_args()
    args.out_dir.mkdir(parents=True, exist_ok=True)

    records: list[dict[str, Any]] = []
    if args.index_jsonl.exists():
        with args.index_jsonl.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                rec = json.loads(line)
                records.append(
                    {
                        "id": rec.get("id"),
                        "title": rec.get("title"),
                        "type": rec.get("source_type"),
                        "path": rec.get("path"),
                        "url": rec.get("url"),
                        "tags": rec.get("tags", []),
                        "content": str(rec.get("content", ""))[:900],
                    }
                )
                if len(records) >= args.max_records:
                    break

    site_index = {
        "records": records,
        "record_count": len(records),
    }
    (args.out_dir / "search-index.json").write_text(
        json.dumps(site_index, ensure_ascii=False),
        encoding="utf-8",
    )

    meta = read_json(args.index_meta)
    manifest = read_yaml(args.manifest)
    verification = read_json(args.verification_json)
    repo_lock = read_json(args.repo_lock_json)
    benchmark_runs = [
        summarize_eval(
            args.retrieval_eval_json,
            "Baseline Retrieval Regression",
            "retrieval",
            "make eval-retrieval",
        ),
        summarize_eval(
            args.agent_eval_json,
            "Baseline Llama Source-Selection",
            "agent",
            "make eval-agent-ollama",
        ),
        summarize_eval(
            args.ollama_retrieval_eval_json,
            "Ollama+Codex Question Set Retrieval",
            "retrieval",
            "make eval-retrieval-ollama-qbank",
        ),
        summarize_eval(
            args.ollama_agent_eval_json,
            "Ollama+Codex Question Set Llama Source-Selection",
            "agent",
            "make eval-agent-ollama-qbank",
        ),
        summarize_eval(
            args.codex_stretch_retrieval_eval_json,
            "Codex Stretch Retrieval",
            "retrieval",
            "make eval-retrieval-codex-stretch",
        ),
        summarize_eval(
            args.codex_stretch_agent_eval_json,
            "Codex Stretch Llama Source-Selection",
            "agent",
            "make eval-agent-ollama-codex-stretch",
        ),
    ]

    latest_eval_ts = max(
        (
            int(run.get("timestamp_unix"))
            for run in benchmark_runs
            if run.get("available") and run.get("timestamp_unix") is not None
        ),
        default=None,
    )

    overview = {
        "index_meta": meta,
        "manifest": {
            "support_docs": len(manifest.get("support_docs", [])) if isinstance(manifest, dict) else 0,
            "repos": len(manifest.get("repos", [])) if isinstance(manifest, dict) else 0,
        },
        "verification": {
            "total_urls": verification.get("total_urls", 0),
            "verified": verification.get("verified", 0),
            "blocked_access": verification.get("blocked_access", 0),
            "needs_review": verification.get("needs_review", 0),
            "errors": verification.get("errors", 0),
        },
        "repo_lock": {
            "total": repo_lock.get("summary", {}).get("total", 0),
            "worktree_present": repo_lock.get("summary", {}).get("worktree_present", 0),
            "mirror_present": repo_lock.get("summary", {}).get("mirror_present", 0),
        },
        "benchmarks": {
            "latest_timestamp_unix": latest_eval_ts,
            "runs": benchmark_runs,
            "tests_run": [
                {
                    "command": "make eval-retrieval",
                    "description": "Lexical retrieval regression on baseline benchmark.",
                },
                {
                    "command": "make eval-agent-ollama",
                    "description": "Llama source-selection on baseline benchmark.",
                },
                {
                    "command": "make gen-questions-ollama",
                    "description": "Generate and curate an Ollama+Codex question bank.",
                },
                {
                    "command": "make eval-retrieval-ollama-qbank",
                    "description": "Retrieval regression on the curated Ollama+Codex benchmark.",
                },
                {
                    "command": "make eval-agent-ollama-qbank",
                    "description": "Llama source-selection on the curated Ollama+Codex benchmark.",
                },
                {
                    "command": "make eval-retrieval-codex-stretch",
                    "description": "Retriever regression on Codex-first stretch benchmark.",
                },
                {
                    "command": "make eval-agent-ollama-codex-stretch",
                    "description": "Llama source-selection on Codex-first stretch benchmark.",
                },
            ],
            "quality_gates": {
                "baseline_retrieval_min": 0.75,
                "baseline_agent_min": 0.70,
                "qbank_retrieval_min": 0.70,
                "qbank_agent_min": 0.60,
                "codex_stretch_retrieval_min": 0.70,
                "codex_stretch_agent_min": 0.60,
            },
        },
    }
    (args.out_dir / "overview.json").write_text(
        json.dumps(overview, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    examples_payload = build_benchmark_examples(
        retrieval_path=args.ollama_retrieval_eval_json,
        agent_path=args.ollama_agent_eval_json,
        question_bank_path=args.ollama_question_bank_yaml,
    )
    (args.out_dir / "benchmark_examples.json").write_text(
        json.dumps(examples_payload, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    print(f"[OK] Wrote site search index: {args.out_dir / 'search-index.json'}")
    print(f"[OK] Wrote site overview: {args.out_dir / 'overview.json'}")
    print(f"[OK] Wrote benchmark examples: {args.out_dir / 'benchmark_examples.json'}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
